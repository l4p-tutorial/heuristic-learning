{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction by Example\n",
    "======================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short example will demonstrate how you can use WLPlan for generating features for planning problems and states which you can then use to train a regression model.\n",
    "\n",
    "A longer example of using WLPlan for training, inference and search in Python is available in this [test file](https://github.com/DillonZChen/wlplan/blob/main/tests/test_train_eval_blocks.py). This notebook only contains the training part.\n",
    "\n",
    "The [GOOSE](https://github.com/DillonZChen/goose) planner provides an optimised usage of WLPlan that implements training in Python, and inference and search in C++.\n",
    "\n",
    "**NOTE** This notebook requires Python 3.10, 3.11 or 3.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that Python version is 3.10, 3.11 or 3.12\n",
    "import sys\n",
    "if sys.version_info < (3, 10) or sys.version_info >= (3, 13):\n",
    "    print(\"This script requires Python 3.10, 3.11, or 3.12\\n\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by installing and importing some Python packages\n",
    "- `pymdzcf`: a [mimir](https://github.com/simon-stahlberg/mimir) fork for generating state successors\n",
    "- `wlplan`: for generating feature embeddings from planning data\n",
    "- `matplotlib`: for simple plots\n",
    "- `numpy`: for representing feature embeddings efficiently for training\n",
    "- `scikit-learn`: for training regression models\n",
    "- `tqdm`: for displaying progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pymimir\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Useful scikit-learn functions\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Useful WLPlan imports\n",
    "import wlplan\n",
    "from wlplan.data import DomainDataset, ProblemDataset\n",
    "from wlplan.feature_generator import init_feature_generator, get_available_feature_generators, get_available_pruning_methods, get_available_graph_generators\n",
    "from wlplan.planning import State, parse_domain, parse_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data\n",
    "The most code intensive part of a machine learning pipeline is usually the handling of data. This is no exception for planning, as you will see that most of the code in this example is spent on parsing data. Here, we parse training data in the form of `(state, optimal_cost_to_go)` pairs using a parser of your choice. We choose to use the [mimir](https://github.com/simon-stahlberg/mimir) for generating state successors but any other method can do as long as the data is eventually represented in a `wlplan.data.DomainDataset` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"blocksworld\"\n",
    "domain_pddl = f\"{domain}/domain.pddl\"\n",
    "\n",
    "wlplan_domain = parse_domain(domain_pddl)\n",
    "mimir_domain = pymimir.DomainParser(str(domain_pddl)).parse()\n",
    "\n",
    "wlplan_data = []\n",
    "y = []\n",
    "\n",
    "# Loop over problems\n",
    "for f in tqdm(sorted(os.listdir(f\"{domain}/training_plans\"))):\n",
    "    problem_pddl = f\"{domain}/training/\" + f.replace(\".plan\", \".pddl\")\n",
    "    plan_file = f\"{domain}/training_plans/\" + f\n",
    "\n",
    "    # Parse problem with mimir\n",
    "    mimir_problem = pymimir.ProblemParser(str(problem_pddl)).parse(mimir_domain)\n",
    "    mimir_state = mimir_problem.create_state(mimir_problem.initial)\n",
    "\n",
    "    name_to_schema = {s.name: s for s in mimir_domain.action_schemas}\n",
    "    name_to_object = {o.name: o for o in mimir_problem.objects}\n",
    "\n",
    "    # Construct wlplan problem\n",
    "    name_to_predicate = {p.name: p for p in wlplan_domain.predicates}\n",
    "    positive_goals = []\n",
    "    for literal in mimir_problem.goal:\n",
    "        assert not literal.negated\n",
    "        mimir_atom = literal.atom\n",
    "        wlplan_atom = wlplan.planning.Atom(\n",
    "            predicate=name_to_predicate[mimir_atom.predicate.name],\n",
    "            objects=[o.name for o in mimir_atom.terms],\n",
    "        )\n",
    "        positive_goals.append(wlplan_atom)\n",
    "\n",
    "    wlplan_problem = parse_problem(domain_pddl, problem_pddl)\n",
    "    \n",
    "    # Collect actions\n",
    "    actions = []\n",
    "    with open(plan_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line.startswith(\";\"):\n",
    "                continue\n",
    "            action_name = line.strip()\n",
    "            action_name = action_name.replace(\"(\", \"\")\n",
    "            action_name = action_name.replace(\")\", \"\")\n",
    "            toks = action_name.split(\" \")\n",
    "            schema = toks[0]\n",
    "            schema = name_to_schema[schema]\n",
    "            args = toks[1:]\n",
    "            args = [name_to_object[arg] for arg in args]\n",
    "            action = pymimir.Action.new(mimir_problem, schema, args)\n",
    "            actions.append(action)\n",
    "\n",
    "    # Collect plan trace states\n",
    "    wlplan_states = []\n",
    "\n",
    "    def mimir_to_wlplan_state(mimir_state: pymimir.State):\n",
    "        atoms = []\n",
    "        for atom in mimir_state.get_atoms():\n",
    "            wlplan_atom = wlplan.planning.Atom(\n",
    "                predicate=name_to_predicate[atom.predicate.name],\n",
    "                objects=[o.name for o in atom.terms],\n",
    "            )\n",
    "            atoms.append(wlplan_atom)\n",
    "        return State(atoms)\n",
    "    \n",
    "    h_opt = len(actions)\n",
    "    wlplan_states.append(mimir_to_wlplan_state(mimir_state))\n",
    "    y.append(h_opt)\n",
    "    for action in actions:\n",
    "        h_opt -= 1\n",
    "        mimir_state = action.apply(mimir_state)\n",
    "        wlplan_states.append(mimir_to_wlplan_state(mimir_state))\n",
    "        y.append(h_opt)\n",
    "\n",
    "    wlplan_data.append(ProblemDataset(problem=wlplan_problem, states=wlplan_states))\n",
    "\n",
    "# This is what we need to feed into our feature generator below\n",
    "dataset = DomainDataset(domain=wlplan_domain, data=wlplan_data)\n",
    "\n",
    "# Save the dataset for future use\n",
    "with open(f\"wlplan-{domain}.pkl\", \"wb\") as f:\n",
    "    pickle.dump((wlplan_domain, dataset, y), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WL Features\n",
    "The following code demonstrates in a matter of lines how to generate matrix embeddings of planning data using WLPlan. Specifically, we implement the pipeline of converting planning problems and states into graphs and embedding the resulting graphs into feature embeddings in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_generator = init_feature_generator(\n",
    "    feature_algorithm=\"wl\",\n",
    "    domain=wlplan_domain,\n",
    "    graph_representation=\"ploig\",\n",
    "    iterations=1,\n",
    "    pruning=\"none\",\n",
    "    multiset_hash=True,\n",
    ")\n",
    "feature_generator.collect(dataset)\n",
    "X = np.array(feature_generator.embed(dataset)).astype(float)\n",
    "y = np.array(y)\n",
    "print(f\"{X.shape=}\")\n",
    "print(f\"{y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some additional hyperparameter options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{get_available_graph_generators()=}\")\n",
    "print(f\"{get_available_feature_generators()=}\")\n",
    "print(f\"{get_available_pruning_methods()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Linear Regression Model\n",
    "\n",
    "The following code demonstrates how we can now just use out of the box ML libraries such as [scikit-learn](https://scikit-learn.org) for training regression models for predicting heuristic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "loss = np.mean((y - y_pred) ** 2)\n",
    "print(f\"{loss=}\")\n",
    "weights = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for visualisation\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "# normalise data\n",
    "X_r -= np.mean(X_r, axis=0)\n",
    "X_r /= np.max(np.abs(X_r), axis=0)\n",
    "max_y = int(round(max(y)))\n",
    "min_y = int(round(min(y)))\n",
    "y = np.array(y)\n",
    "\n",
    "# plot\n",
    "cmap = plt.get_cmap(\"hsv\", max_y - min_y + 1)\n",
    "for i in range(min_y, max_y + 1):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=cmap(i), alpha=0.8)\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "# display colour scale\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min_y, vmax=max_y))\n",
    "sm.set_array([])\n",
    "ax = plt.gca()\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"h* value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Learned Model\n",
    "\n",
    "Saving a model is easy with WLPlan. We store the weights of the model and then serialise the entire feature generator to a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_generator.set_weights(weights)\n",
    "feature_generator.save(f\"{domain}.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve on this script and try and learn a stronger model for the Blocksworld domain. This can be done in several ways, including but not limited to\n",
    "- changing the hyperparameters \n",
    "- generating new data or using the data in new ways\n",
    "- changing the optimisation criterion\n",
    "\n",
    "You can evaluate your models with the script `evaluate.py` that performs heuristic search with the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
